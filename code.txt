

Input Dataset :-

https://www.kaggle.com/yelp-dataset/yelp-dataset

Environment:-
 Here I have used HDP community cluster of 20 nodes.


I are using the yelp user dataset which has the total record count of 1326100

I have created a hbase table "yelp_user_testing" which contains 2 column-families 
	1)userdetails
	2)compliments


hbase-command :- create 'yelp_user_testing','userdetails','compliments'


Then i have created a hive table which is using the HBaseStorageHandler and pointing to hbase table , 
for this i have add some hbase-hive jars to hive shell and created the hive table

once i load the data into this hive table it will internally store the data into hbase and 
when we write a select it will read the data from hbase.
we are user user_id and row_key and if new record with same user_id is inserted it will update the existing values.



Hive-shell :- 

ADD JAR /usr/hdp/current/hive-client/lib/zookeeper-3.4.6.2.6.5.0-292.jar;
ADD JAR /usr/hdp/current/hive-client/lib/hive-hbase-handler.jar;
ADD JAR /usr/hdp/current/hive-client/lib/guava-14.0.1.jar;
ADD JAR /usr/hdp/current/hbase-client/lib/hbase-client.jar;
ADD JAR /usr/hdp/current/hbase-client/lib/hbase-common.jar;
ADD JAR /usr/hdp/current/hbase-client/lib/hbase-hadoop2-compat.jar;
ADD JAR /usr/hdp/current/hbase-client/lib/hbase-hadoop-compat.jar;
ADD JAR /usr/hdp/current/hbase-client/lib/hbase-protocol.jar;
ADD JAR /usr/hdp/current/hbase-client/lib/hbase-server.jar;
ADD JAR /usr/hdp/2.5.0.0-1245/hadoop/client/htrace-core.jar;
 
 CREATE EXTERNAL TABLE default.yelp_user_hbase
 (key string,
  name string,
  review_count string,
  yelping_since string,
  friends string,
  useful string,
  funny   string,
  cool string,
  fans string,
  elite string,
  average_stars string,
  compliment_hot   string,
  compliment_more string,
  compliment_profile string,
  compliment_cute string,
  compliment_list   string,
  compliment_note string,
  compliment_plain string,
  compliment_cool string,
  compliment_funny   string,
  compliment_writer string,
  compliment_photos string
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES 
("hbase.columns.mapping" = ":key,userdetails:name ,userdetails:review_count ,userdetails:yelping_since ,userdetails:friends ,userdetails:useful ,userdetails:funny   ,userdetails:cool ,userdetails:fans ,userdetails:elite ,compliments:average_stars ,compliments:compliment_hot   ,compliments:compliment_more ,compliments:compliment_profile ,compliments:compliment_cute ,compliments:compliment_list   ,compliments:compliment_note ,compliments:compliment_plain ,compliments:compliment_cool ,compliments:compliment_funny   ,compliments:compliment_writer ,compliments:compliment_photos")
TBLPROPERTIES ("hbase.table.name" = "yelp_user_testing");

Next-step is to load the initial dataset of 1.3 million records in to Hbase for that i have used spark to load the data into the hive-hbase table.

location of inital-file :- /public/yelp-dataset/yelp_user.csv

Command:-

spark-shell --jars /usr/hdp/current/hive-client/lib/zookeeper-3.4.6.2.6.5.0-292.jar,/usr/hdp/current/hive-client/lib/hive-hbase-handler.jar,/usr/hdp/current/hive-client/lib/guava-14.0.1.jar,/usr/hdp/current/hbase-client/lib/hbase-client.jar,/usr/hdp/current/hbase-client/lib/hbase-common.jar,/usr/hdp/current/hbase-client/lib/hbase-hadoop2-compat.jar,/usr/hdp/current/hbase-client/lib/hbase-hadoop-compat.jar,/usr/hdp/current/hbase-client/lib/hbase-protocol.jar,/usr/hdp/current/hbase-client/lib/hbase-server.jar,/usr/hdp/2.5.0.0-1245/hadoop/client/htrace-core.jar

 var user = spark.read.format("csv").option("header","true").load("/public/yelp-dataset/yelp_user.csv")
  user.registerTempTable("input")
 spark.sql("insert into default.yelp_user_hbase select * from input")
 
 
For verification I have checked the count of records in hive table:-
 
 
 
hive (default)> select count(*) from default.yelp_user_hbase;

		Total jobs = 1
		Launching Job 1 out of 1
		Number of reduce tasks determined at compile time: 1
		In order to change the average load for a reducer (in bytes):
		  set hive.exec.reducers.bytes.per.reducer=<number>
		In order to limit the maximum number of reducers:
		  set hive.exec.reducers.max=<number>
		In order to set a constant number of reducers:
		  set mapreduce.job.reduces=<number>
		Kill Command = /usr/hdp/2.6.5.0-292/hadoop/bin/hadoop job  -kill job_1565300265360_25491
		Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
		2019-10-06 15:34:11,051 Stage-1 map = 0%,  reduce = 0%
		2019-10-06 15:34:25,663 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 22.94 sec
		2019-10-06 15:34:28,772 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.52 sec
		2019-10-06 15:34:30,854 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.61 sec
		MapReduce Total cumulative CPU time: 27 seconds 610 msec
		Ended Job = job_1565300265360_25491
		MapReduce Jobs Launched:
		Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 27.61 sec   HDFS Read: 38914 HDFS Write: 8 SUCCESS
		Total MapReduce CPU Time Spent: 27 seconds 610 msec
		OK
		1326100
		Time taken: 40.945 seconds, Fetched: 1 row(s)

 
Incremantal data file has 100 records which has updated values(for testing i took 100 records and updated the review_count column value by 10):-
 
  var data_update = spark.read.format("csv").option("header","true").load("/user/wro96/data_100_csv_new_1/")
  data_update.registerTempTable("update")
  spark.sql("insert into default.yelp_user_hbase select * from update")
  
after this command i have check the count of records and its still same because all the incremental data  entries are exiting records which need to be updated.

and then I am attaching 3 files which has the values from the initial load , incremental data input and after the dataload

for the incremental update of 100 records outof 1.3 million it took hardly 1 min.



  
  
  

 
 
 